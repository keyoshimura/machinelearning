{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''やること\n",
    "Amazon sagemakerで最近投入された「Random Cut Forest」をやってみよう(異常値検出に使います)\n",
    "Amazonのハンズオンをそのままコピペっている。\n",
    "https://aws.amazon.com/jp/blogs/news/use-the-built-in-amazon-sagemaker-random-cut-forest-algorithm-for-anomaly-detection/\n",
    "'''\n",
    "\n",
    "\n",
    "'''アルゴリズムの流れ（概要）\n",
    "①データセットからサブセットを抽出する\n",
    "（「Resevoir sampling」というアルゴリズムを使って抽出する）\n",
    "②決定木を複数個生成する\n",
    "③各データポイントの「深さ」の平均値を使って「異常度スコア」を計算する\n",
    "\n",
    "上記の通り「isolation forest」と似ているが、「分岐させる際の特徴量」を選択する際に「各特徴量の分散に基づいて重みをつける」といった点がiForestと異なる。\n",
    "（iForestは完全ランダム）\n",
    "（各特徴量の閾値はランダムに選択するので、そこはiForestと同じ）\n",
    "\n",
    "ちなみに、教師なし学習です。\n",
    "'''\n",
    "\n",
    "\n",
    "'''iForestよりも優位な点\n",
    "①データサンプリングがより早い（「Resevoir sampling」というアルゴリズムを使っている）\n",
    "②iForestでは、特徴量が少なくてその特徴量が異常を見分けるのに不要な際に不具合がある。（RRCFでは、特徴量選択に分散に基づく重み付けをしているのでクリア）\n",
    "'''\n",
    "\n",
    "'''参考文献\n",
    "・元論文\n",
    "https://d1.awsstatic.com/whitepapers/kinesis-anomaly-detection-on-streaming-data.pdf\n",
    "\n",
    "・概要を理解するだけなら、AWSの解説もわかりやすいです\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/rcf_how-it-works.html\n",
    "'''\n",
    "\n",
    "\n",
    "'''パラメータ\n",
    "下記を参照。\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/rcf_hyperparameters.html\n",
    "\n",
    "num_samples_per_tree:各決定木に投入されるデータサイズ。多い方が計算結果が安定するものの、処理に時間がかかる。\n",
    "　　　　　　　　　　　 　　　　　　　　　　　　　　　　　　　　　　　　　　　　（1/num_samples_per_tree）が「想定する異常データの割合」に近いことを想定している。\n",
    "num_trees:決定木の数。多い方が計算結果が安定するものの、処理に時間がかかる。\n",
    " 　　　　　　　　　　　　　　　　　　AWSはとりあえず初めは100個くらいで始めたらいいのでは、と言っている。（精度や複雑さを考慮すると、最初はとりあえずこのくらい、という意味）\n",
    "　　　　　　 　　　　　　　　　　　　最終的には、このパラメータもアプリケーションによって最適値が異なるのでチューニングが必要。\n",
    "feature_dim:利用する特徴量の割合？チュートリアルだと「feature_size」という名前だったが、これで実行すると「feature_dimが指定されていない 」とエラーが出たのでパラメータ名を変更した。\n",
    "（shingle_size):シングリングのサイズ。これもいじる必要がある。任意？\n",
    "'''\n",
    "\n",
    "\n",
    "'''アルゴリズムの詳細\n",
    "①AWSは「Random Cut Forest」と言っているが参照論文を見ると「Robust Random Cut Forest」アルゴリズムを利用しているようです。\n",
    "②新しいデータポイントが投入された際は、再度初めから計算をやり直す、という形になる\n",
    "③シングリング（shingling）という前処理テクニックを使うよ。\n",
    "　 　ストリームデータを扱う際の手法。時系列データなら、連続する４つの1次元データをひとまとめにして４次元のデータとして扱う、といった手法。\n",
    "   小さなノイズをフィルタリングする、データの周期性をよく捉えられるようにする、といった特徴がある。\n",
    "④異常かどうかを判断する閾値は「F1値」を最適化するようにして生成する\n",
    "(精度とリコールの調和平均)\n",
    "⑤モデルから対象のデータを削除、追加するのも簡単にできるとのこと。\n",
    "（あたかもそれらのデータが最初からなかったかのように、またはあったかのようにできる、ということ）\n",
    "（なので、後から「このデータがサンプルに入っていたらどんな結果だったかな」というのも簡単に検証できる）\n",
    "⑥決定木は２分岐、各ノードに１つずつデータが分類されるまで成長させる\n",
    "⑦各データポイントの「異常度スコア」は、全決定木における深さの平均で取られる\n",
    "⑧データが異常かどうかを判断する閾値については、利用ケースによって異なる。一般的には３標準偏差離れたら、でいいのではないでしょうか？\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''1.Amazon S3 でデータを取得、検査、保存する\n",
    "'''\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "import urllib.request\n",
    "\n",
    "data_filename = 'nyc_taxi.csv'\n",
    "data_source = 'https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/nyc_taxi.csv'\n",
    "\n",
    "urllib.request.urlretrieve(data_source, data_filename)\n",
    "taxi_data = pandas.read_csv(data_filename, delimiter=',')\n",
    "taxi_data.plot(title='Taxi Ridership in NYC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2.CSV 形式のデータを変換し、そのデータを Amazon S3 バケットへプッシュしています。\n",
    "'''\n",
    "\n",
    "def convert_and_upload_training_data(\n",
    "    ndarray, bucket, prefix, filename='data.pbr'):\n",
    "    import boto3\n",
    "    import os\n",
    "    from sagemaker.amazon.common import numpy_to_record_serializer\n",
    "\n",
    "    # convert Numpy array to Protobuf RecordIO format\n",
    "    serializer = numpy_to_record_serializer()\n",
    "    buffer = serializer(ndarray)\n",
    "\n",
    "    # upload to S3\n",
    "    s3_object = os.path.join(prefix, 'train', filename)\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(s3_object).upload_fileobj(buffer)\n",
    "    s3_path = 's3://{}/{}'.format(bucket, s3_object)\n",
    "    return s3_path\n",
    "\n",
    "bucket = 'cm-yoshim-sagemaker' # <-- use your own bucket, here\n",
    "prefix = 'robust_randum_cut_forest'\n",
    "s3_train_data = convert_and_upload_training_data(\n",
    "    taxi_data.value.as_matrix().reshape(-1,1),\n",
    "    bucket,\n",
    "    prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''3.モデル生成！\n",
    "'''\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# コンテナの一覧\n",
    "containers = {\n",
    "    'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/randomcutforest:latest',\n",
    "    'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/randomcutforest:latest',\n",
    "    'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/randomcutforest:latest',\n",
    "    'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/randomcutforest:latest'}\n",
    "region_name = boto3.Session().region_name\n",
    "container = containers[region_name]\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# 予測に用いるインスタンスの設定\n",
    "rcf = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    sagemaker.get_execution_role(),\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c5.xlarge',\n",
    "    sagemaker_session=session)\n",
    "\n",
    "# robust_random_cut_forestのハイパーパラメータを設定(チュートリアルとは「feature_dim」パラメータの名前が異なる。)\n",
    "rcf.set_hyperparameters(\n",
    "    num_samples_per_tree=200,\n",
    "    num_trees=50,\n",
    "    feature_dim=1)\n",
    "\n",
    "# inputデータの指定\n",
    "s3_train_input = sagemaker.session.s3_input(\n",
    "    s3_train_data,\n",
    "    distribution='ShardedByS3Key',\n",
    "    content_type='application/x-recordio-protobuf')\n",
    "\n",
    "# モデル生成\n",
    "rcf.fit({'train': s3_train_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''4.異常スコアを予測する\n",
    "'''\n",
    "\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "# 推論エンドポイントを作成\n",
    "rcf_inference = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    ")\n",
    "\n",
    "rcf_inference.content_type = 'text/csv'\n",
    "rcf_inference.serializer = csv_serializer\n",
    "rcf_inference.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''5.異常スコアを取得\n",
    "トレーニングセット全体で推論を実行する。\n",
    "'''\n",
    "\n",
    "results = rcf_inference.predict(taxi_data.value.as_matrix().reshape(-1,1))\n",
    "scores = [datum['score'] for datum in results['scores']]\n",
    "taxi_data['score'] = pandas.Series(scores, index=taxi_data.index)\n",
    "\n",
    "score_mean = taxi_data.score.mean()\n",
    "score_std = taxi_data.score.std()\n",
    "\n",
    "score_cutoff = score_mean + 3*score_std # 平均スコアよりも3標準偏差離れているポイントを異常値としてみなす\n",
    "anomalies = taxi_data[taxi_data['score'] > score_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''6.予測結果の可視化\n",
    "異常値のデータポイントをハイライトして可視化する\n",
    "'''\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(taxi_data['value'], alpha=0.8)\n",
    "ax1.set_ylabel('Taxi Ridership', color='C0')\n",
    "ax1.tick_params('y', colors='C0')\n",
    "\n",
    "ax2.plot(taxi_data['score'], color='C1')\n",
    "ax2.plot(anomalies.index, anomalies.score, 'ko')\n",
    "ax2.set_ylabel('Anomaly Score', color='C1')\n",
    "ax2.tick_params('y', colors='C1')\n",
    "\n",
    "fig.suptitle('Taxi Ridership in NYC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''memo\n",
    "①F値、リコール、精度等のいろいろな基準でモデルを評価できそうなので、色々いじって確かめてみよう。\n",
    "②元論文を読んでみたところ、「時系列データで異常値を見る」という点では、「スタート」と「エンド」を抽出できている。\n",
    "（途中の部分は異常値が大きくなる感じではない）\n",
    "（iForestでは、「エンド」は抽出できているが「スタート」は抽出できていない）\n",
    "③元論文では、iForestと精度を比較している。結果としてはRRCFの方が良い、というものでありこの時はRRCFでは決定木を200個、各決定木に渡すデータサイズを1000にしている\n",
    "（データサイズを変えても比較結果に影響はない、と論文では言っている）\n",
    "④iForestと比較すると「positive precision」がとても向上していた。\n",
    "⑤処理時間もiForestよりも早いので、「異常発生時に早くアラームをあげる」ことも可能と言っている\n",
    "（論文で利用しているタクシーデータで、RRCFでは７時間、iForestは11時間で処理が完了している）\n",
    "（ただ、上記の時間は１からモデルを作成する場合、なので異常発生時にアラームを早くあげるという点については意味のない検証である）\n",
    "⑥時系列データでは「何分ごとにデータを取得するか」が非常に大事。（今回のタクシーデータでは30分ごと）\n",
    "これは業務知識が必要となる。\n",
    "このレンジが小さすぎるとしょっちゅう異常と判断して誤ったアラートをあげてしまうし、大きすぎると異常を見逃してしまう。\n",
    "一方、各決定木に渡すデータサイズを変更することは結果にそこまで影響しない。\n",
    "⑦論文の最後は「RRCFではデータの変化点を見つけることもできそう」と言っている。\n",
    "⑧データポイントの置換については、似たようなデータを置換すると計算コストが小さくなる、とのこと\n",
    "（データポイントを削除した際のモデル再計算については、分散や元データから計算できるので改めて再計算する必要がない）\n",
    "⑨論文では、iForestの悪い点についても述べている。\n",
    "　　a.特徴量をランダムに選んでいるせいで、「モデル生成のたびに結果が大きく異なる」こととなる可能性がある\n",
    "  （しかしながら、特徴量が多くなっても柔軟に対応できる点はメリットである、とも述べている）\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

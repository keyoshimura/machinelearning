{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●optimizerのまとめ\n",
    "\n",
    "勾配を計算するoptimizerについて、色々あるけどしょっちゅう忘れるのでメモ\n",
    "基本的に下記の論文をそのまま日本語にした感じ。\n",
    "\n",
    "[Incorporating Nesterov Momentum into Adam](http://cs229.stanford.edu/proj2015/054_report.pdf)\n",
    "\n",
    "なお、optimizerはモデルやアプリケーションによって最適なものが異なるので、一概に「これを使えばOK」と言ったものはまだない。\n",
    "また、実務で機械学習をするにあたり、**「目的を達成するためにはoptimizerをどうこうするよりもアーキテクチャーを変える方が圧倒的に有効」**である点は忘れてはいけない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.勾配降下法\n",
    "大まかに下記の３つがある。\n",
    "\n",
    "\n",
    "### 1-1.最急降下法（Gradient Descent)   \n",
    "最も単純な勾配法。  \n",
    "難しく考えなくていい、数式の通り「勾配方向に進む」形でパラメータを更新するだけ。  \n",
    "その心は、「勾配ベクトルがコスト関数を小さくする方向を示してくれているから」というもの。    \n",
    "\n",
    "  \n",
    "更新する重みパラメータをW、学習率をη、コスト関数をLとすると、重みWを更新する式は下記の通り。  \n",
    "（←は右の式で左を更新する、という意味）  \n",
    "\n",
    "$$g_t =  - \\eta_{t-1}\\frac{\\partial L}{\\partial W_{t-1}}$$  \n",
    "$$W_t ← W_{t-1} + g_t$$\n",
    "\n",
    "\n",
    "上記の数式を日本語にすると下記のイメージ\n",
    "$$重みパラメータ_t　← 重みパラメータ_{t-1} + (学習率_{t-1}*勾配ベクトル_{t-1})$$\n",
    "\n",
    "端的に言うと「勾配ベクトルに学習率をかけたものを足し合わせる」ような更新をしている。  \n",
    "学習率にどのような値を設定するかが肝で、大きくすれば毎回の重み更新で大きく値を更新できるので早く学習が収束するかもしれないが、  \n",
    "一方、いつまでも収束しない可能性もある。\n",
    "\n",
    "##### メリット\n",
    "アルゴリズムが理解しやすく、実装が単純\n",
    "\n",
    "##### デメリット\n",
    "全てのデータの誤差を計算した後にパラメータを更新するので、計算量が多い。\n",
    "また、コスト関数が複雑な形をしていると局所解に陥りやすい。  \n",
    "（鞍点にハマりやすい）\n",
    "\n",
    "##### 余談 \n",
    "勾配ベクトルが「コスト関数を小さくする」方向を示しているものの、負の値となっているので、−1をかけて正しく「コスト関数を小さくする方向」の勾配ベクトルを足し合わせている。  \n",
    "学習率（η）をハイパーパラメータとして設定する必要があるが、これの調整が難しい。\n",
    "\n",
    "##### 参照\n",
    "[wiki](https://ja.wikipedia.org/wiki/%E6%9C%80%E6%80%A5%E9%99%8D%E4%B8%8B%E6%B3%95)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1-2.確率的勾配降下法（Stochastic Gradient Descent - SDG）  \n",
    "基本的には「最急降下法」と同じ。  \n",
    "SGDは「訓練データを1つだけ取り出して誤差を計算し、パラメーターを更新する」というものなので「最急降下法」よりも計算量が少ない。  \n",
    "\n",
    "結構よく使われているみたいです。\n",
    "\n",
    "\n",
    "### 1-3.ミニバッチ確率的勾配降下法（Minibatch SGD - MSGD）  \n",
    "「最急降下法」と「確率的勾配降下法」の中間のようなイメージ。  \n",
    "あらかじめ決めたサイズ（バッチサイズという）で誤差を計算して、パラメータを更新する\n",
    "\n",
    "ディープラーニングの最適化でもミニバッチはよく使われているみたいですね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Momentum  \n",
    "\n",
    "### 2-1.普通のMomentum\n",
    "ほとんどが「勾配降下法」と同じで、「慣性」のような概念が追加されたイメージ。  \n",
    "ここで言う「慣性」とは、「前回大きくパラメータを更新したなら、次も大きく更新する」といったもの。  \n",
    "（逆に言うと、更新分が小さいようなら次の更新分も小さくなる）  \n",
    "数式的に言うと下記。  \n",
    "\n",
    "$$g_t ← \\frac{\\partial L}{\\partial W_{t-1}}$$  \n",
    "$$m_t ←  µm_{t−1} + g_t $$  \n",
    "$$W_t ← W_{t-1}- \\eta m_t$$\n",
    "\n",
    "mが「前回の更新量をどれだけ次回の更新に反映させるか」といったニュアンスの「慣性」の位置付け。  \n",
    "なので、**「今まで更新し続けたパラメータは次回も更新しやすい」**ものになる\n",
    "\n",
    "### 2-2.（Nesterov’s Accelerated Gradient - NAG）\n",
    "ちょっと複雑だし、あまり使われていない。  \n",
    "\n",
    "$$g_t ← \\frac{\\partial L}{\\partial (W_{t-1}-ηµm_{t-1})}$$  \n",
    "$$m_t ← µm_{t−1} +g_t$$  \n",
    "$$W_t ← W_{t−1} −ηm_t$$\n",
    "\n",
    "\n",
    "### 2-3.勾配降下法との違い\n",
    "\n",
    "・慣性のような概念が追加されたので「SGDのようなジグザグしたパラメータ探索」をしなくなる  \n",
    "・$m*g_{t-1}$の分だけ重みパラメータの更新量が大きくなる。  \n",
    "（SGDと比較すると、学習率を少し下げた方がいいかもしれません）  \n",
    "・\n",
    "\n",
    "### 2-4.Momentum SGD\n",
    "MomentumとSGDの概念を組み合わせたoptimizerもよく使われているみたいですね。\n",
    "基本的には上記の「Momentum」の計算式に基づきますが、「訓練データ１つから、誤差を計算しパラメータを更新する」というフローになる。\n",
    "\n",
    "\n",
    "### 2-5.メリット\n",
    "学習の収束化が勾配降下法よりも早い、らしいが実証できていないし数式的な証明も見ていないのでよくわからない。\n",
    "\n",
    "### 2-6.デメリット\n",
    "\n",
    "\n",
    "### 余談\n",
    "µを大きく設定すると「慣性が強くなる」イメージなので「新しいパラメータの移動に対して鈍感」になる。  \n",
    "基本的にµは１より小さい値を設定する。\n",
    "調整するパラメータが２つ（η、µ）あるので、パラメータチューニングが難しい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.AdaGrad  \n",
    "\n",
    "### 3-1.概要\n",
    "過去の勾配の値を累積し、それに基づいて全体的に学習を適応させる、といったもの。  \n",
    "つまり、学習係数を自動で調整してくれる。  \n",
    "$g_t$は勾配ベクトルで、$g_t^2$はアダマール積。\n",
    "$r$が勾配を蓄積するベクトル。\n",
    "\n",
    "$$g_t ← \\frac{\\partial L}{\\partial W_{t-1}}$$  \n",
    "$$r_t ← r_{t-1} + g_t^2$$  \n",
    "$$W_i ← W_{i-1} - η\\frac{g_t}{δ + r_t^{1/2}}$$\n",
    "\n",
    "\n",
    "δは分母が０にならないようにとても小さい値を足してあげる。  \n",
    "($10^{-7}とか$）\n",
    "\n",
    "\n",
    "### 3-2.パラメータ\n",
    "η:学習率  \n",
    "処理中はいい感じに上記の式で調整してくれるが、とはいえ最初はちゃんと設定してあげる必要がありますよ。\n",
    "\n",
    "### 3-3.余談\n",
    "$δ + r_t^{1/2}$はブロードキャストされた状態で$g_t$に各要素ごとに適用される。  \n",
    "（アダマール積）\n",
    "\n",
    "数式を見ると、勾配が大きいパラメータの学習率を急速に減少させる傾向がある。  \n",
    "それに付随してか、AdaGradは早まって学習率を減少させてしまう傾向があるそうです。  \n",
    "この欠点の解消を目指したオプティマイザが「RMSProp」らしい。\n",
    "（参考文献[実践Deep Learning](https://www.amazon.co.jp/%E5%AE%9F%E8%B7%B5-Deep-Learning-%E2%80%95Python%E3%81%A8TensorFlow%E3%81%A7%E5%AD%A6%E3%81%B6%E6%AC%A1%E4%B8%96%E4%BB%A3%E3%81%AE%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0-%E3%82%AA%E3%83%A9%E3%82%A4%E3%83%AA%E3%83%BC%E3%83%BB%E3%82%B8%E3%83%A3%E3%83%91%E3%83%B3/dp/4873118328)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.RMSProp\n",
    "\n",
    "### 4-1.概要\n",
    "Adagradでは、勾配が大きいパラメータの学習率を急速に減少させる傾向があるため、極小値に陥りやすい。  \n",
    "これへの対応として、AdaGradに「古い勾配の影響度を小さくする」概念を追加したアルゴリズムを**RMSProp**という。  \n",
    "数式的は下記の通りで、**AdaGradとの違いは、二つ目の式の部分だけ**。  \n",
    "\n",
    "\n",
    "$$g_t ← \\frac{\\partial L}{\\partial W_{t-1}}$$  \n",
    "$$r_t ← ρr_{t-1} + (1- ρ)g_t^2$$  \n",
    "$$W_i ← W_{i-1} - η\\frac{g_t}{δ + r_t^{1/2}}$$\n",
    "\n",
    "\n",
    "\n",
    "AdaGradとは異なり、直近の勾配の影響をより重視したoptimizerです。  \n",
    "ρは「減衰係数」と言われ、大きいほど古い勾配の値を保持する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Adam（adaptive moment estimation)\n",
    "\n",
    "### 5-1.概要\n",
    "MomentumとRMSPropを組み合わせたようなアルゴリズム。  \n",
    "2015年に発表された、割と新しいアルゴリズム。\n",
    "\n",
    "\n",
    "$g_t ← \\frac{\\partial L}{\\partial W_{t-1}}$・・・①いつも通りの勾配計算  \n",
    "$m_t ← µm_{t−1} + (1− µ)g_t$・・・②勾配に対してのモーメンタム保持  \n",
    "$\\hat{m_t} ← \\frac{m_t}{1−µ^t}$・・・③モーメンタムを少し修正  \n",
    "  \n",
    "$r_t ← ρr_{t−1} + (1−ρ)g_t^2$・・・④RMSProp的に、勾配を保持  \n",
    "$\\hat{r_t} ← \\frac{r_t}{(1−ρ^t)}$・・・⑤RMSPropの値を少し修正  \n",
    "$W_t ← W_{t−1} −η\\frac{\\hat{m_t}}{ \\hat{r_t}^{1/2}+ε}$・・・⑥重みパラメータ更新(③、⑤を利用)  \n",
    "\n",
    "\n",
    "### 5-2.指定するパラメータ\n",
    "下記の3つ。\n",
    "\n",
    "µ:momentum係数。１以下に設定することで収束させることが基本。（③の式を見ればわかるが、これで収束させる）  \n",
    "ρ:減衰係数１以下に設定することで収束させることが基本。（⑤の式を見ればわかるが、これで収束させる）  \n",
    "ε:分母が0にならないように足し合わせてあげる数字。$10^{-7}$とかでいいんじゃないでしょうか？\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
